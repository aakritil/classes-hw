{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFM_7bHD-La0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math\n",
        "\n",
        "class NaiveClassifer:\n",
        "  # here we define the attributes of the class\n",
        "  traincsv = \"\"\n",
        "  testcsv = \"\"\n",
        "  traincols = []\n",
        "  yprobs = []\n",
        "  condprob0 = {}\n",
        "  condprob1 = {}\n",
        "  \n",
        "\n",
        "  def __init__(self, traincsv, testcsv, traincols):\n",
        "    # initialize each object given the three parameters and make sure to reset values\n",
        "    # call the train function to populate the two dicts and list w the conditional probabilites\n",
        "    self.traincsv = traincsv\n",
        "    self.testcsv = testcsv\n",
        "    self.traincols = traincols\n",
        "    self.condprob0 = {}\n",
        "    self.condprob1 = {}\n",
        "    self.yprobs = []\n",
        "    self.train()\n",
        "\n",
        "  def train(self):\n",
        "    train = pd.read_csv(self.traincsv)\n",
        "\n",
        "    #calculate the approx values of P(Y=0) and P(Y=1)\n",
        "    py_0 =train['Label'].value_counts()[0]/len(train)\n",
        "    py_1 =train['Label'].value_counts()[1]/len(train)\n",
        "\n",
        "    print(f'The probability for P(Y=1) is {py_1}')\n",
        "\n",
        "    self.yprobs= [py_0,py_1]\n",
        "\n",
        "    #calculate the conditional probabilties using the MAP estimate, and save in the dictionaries\n",
        "    for col in self.traincols:\n",
        "      px0_y0 = (len(train[(train[col] == 0) & (train[\"Label\"] == 0)]) + 1) / (train['Label'].value_counts()[0] + 2)\n",
        "      px1_y0 = (len(train[(train[col] == 1) & (train[\"Label\"] == 0)]) + 1) / (train['Label'].value_counts()[0] + 2)\n",
        "\n",
        "      px0_y1 = (len(train[(train[col] == 0) & (train[\"Label\"] == 1)]) + 1) / (train['Label'].value_counts()[1] + 2)\n",
        "      px1_y1 = (len(train[(train[col] == 1) & (train[\"Label\"] == 1)]) + 1) / (train['Label'].value_counts()[1] + 2)\n",
        "      \n",
        "      self.condprob0[col] = [px0_y0,px1_y0]\n",
        "      self.condprob1[col] = [px0_y1,px1_y1]\n",
        "\n",
        "  def predict(self):\n",
        "    test = pd.read_csv(self.testcsv)\n",
        "    predictions = []\n",
        "\n",
        "    #using the values in the dictionary, add the log transformed conditional probabilties to find which\n",
        "    # y value has the higher probability, and choose this y-value as the prediction\n",
        "    for i in range(0, len(test)):\n",
        "      p_y0 = math.log(self.yprobs[0])\n",
        "      p_y1 = math.log(self.yprobs[1])\n",
        "      for col in self.traincols:\n",
        "        if col in test.columns:\n",
        "          if test[col][i]== 0:\n",
        "            p_y0 += math.log(self.condprob0[col][0])\n",
        "            p_y1 += math.log(self.condprob1[col][0])\n",
        "          if test[col][i] == 1:\n",
        "            p_y0 += math.log(self.condprob0[col][1])\n",
        "            p_y1 += math.log(self.condprob1[col][1])\n",
        "\n",
        "      if p_y0 > p_y1 :\n",
        "        predictions.append(0)\n",
        "      else:\n",
        "        predictions.append(1)\n",
        "    \n",
        "    test[\"prediction\"] = predictions\n",
        "\n",
        "    #print confusion matrix and accuracy\n",
        "    cm = confusion_matrix(test[\"Label\"], predictions)\n",
        "\n",
        "    print(cm)\n",
        "    print(f'Accuracy is {sklearn.metrics.accuracy_score(test[\"Label\"], predictions)}')\n",
        "\n",
        "    #calculating parity:\n",
        "    d0 = (len(test[(test[\"Demographic\"] == 0) & (test[\"prediction\"] == 1)]) + 1) / (len(test[test[\"Demographic\"] == 0]) +2)\n",
        "    d1 = (len(test[(test[\"Demographic\"] == 1) & (test[\"prediction\"] == 1)]) + 1) / (len(test[test[\"Demographic\"] == 1]) +2)\n",
        "\n",
        "    print(f\"Parity Calculation: When D=0: {d0}. When D=1: {d1}.\")\n",
        "\n",
        "    sum0 = 0\n",
        "    sum1 = 0\n",
        "    for i in range(len(test)):\n",
        "      if test[\"prediction\"][i] == test[\"Label\"][i] and test[\"Demographic\"][i] == 0:\n",
        "        sum0 += 1\n",
        "      elif test[\"prediction\"][i] == test[\"Label\"][i] and test[\"Demographic\"][i] == 1:\n",
        "        sum1 += 1\n",
        "\n",
        "    #calculating calibration:\n",
        "    c0 =  (sum0 + 1) / (len(test[test[\"Demographic\"] == 0]) +2)\n",
        "    c1 =  (sum1 + 1) / (len(test[test[\"Demographic\"] == 1]) +2)\n",
        "\n",
        "    print(f\"Calibration Calculation: When D=0: {c0}. When D=1: {c1}.\")\n",
        "\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16HAtZ8TBhgo",
        "outputId": "cc160334-1613-4b99-cbde-370c5535ca9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability for P(Y=1) is 0.5\n"
          ]
        }
      ],
      "source": [
        "simpletest = pd.read_csv(\"/content/simple-test.csv\")\n",
        "simpletrain = pd.read_csv(\"/content/simple-train.csv\")\n",
        "\n",
        "simpleclassifier = NaiveClassifer(\"/content/simple-train.csv\", \"/content/simple-test.csv\", ['x1', 'x2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loibjkYRCdvS"
      },
      "outputs": [],
      "source": [
        "predictions = simpleclassifier.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBzNqTCMCgAl"
      },
      "outputs": [],
      "source": [
        "ancestry = pd.read_csv(\"/content/ancestry-train.csv\")\n",
        "ancestrytest = pd.read_csv(\"/content/ancestry-test.csv\")\n",
        "ancestrytest.columns = ancestry.columns\n",
        "ancestrytest.to_csv(\"/content/newancestry.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UU9cXTmMl25"
      },
      "outputs": [],
      "source": [
        "ancestry = NaiveClassifer(\"/content/ancestry-train.csv\", \"/content/newancestry.csv\", ['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8',\n",
        "       'col9', 'col10', 'col11', 'col12', 'col13', 'col14', 'col15', 'col16',\n",
        "       'col17', 'col18', 'col19'])\n",
        "results = ancestry.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq_4jFrxMzs4",
        "outputId": "ec99ec53-f3e2-4af0-abd9-3b178f454ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability for P(Y=1) is 0.5\n",
            "[[ 10   5]\n",
            " [ 42 130]]\n",
            "Accuracy is 0.7486631016042781\n",
            "Parity Calculation: When D=0: 0.5106382978723404. When D=1: 0.7847222222222222.\n",
            "Calibration Calculation: When D=0: 0.5957446808510638. When D=1: 0.7916666666666666.\n"
          ]
        }
      ],
      "source": [
        "heart = NaiveClassifer(\"/content/heart-train.csv\", \"/content/heart-test.csv\",['A.1', 'A.2', 'A.3', 'A.4', 'B.1', 'B.2', 'B.3', 'B.4', 'B.5', 'C.1',\n",
        "       'C.2', 'C.3', 'C.4', 'C.5', 'D.1', 'D.2', 'D.3', 'D.4', 'E.1', 'E.2',\n",
        "       'E.3', 'E.4'])\n",
        "heartpredict = heart.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV_nbh-xKuuR",
        "outputId": "ecd48f76-07dc-4fa7-dd6d-f12014fd40cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability for P(Y=1) is 0.59\n",
            "[[265   0]\n",
            " [234   1]]\n",
            "Accuracy is 0.532\n",
            "Parity Calculation: When D=0: 0.003787878787878788. When D=1: 0.008333333333333333.\n",
            "Calibration Calculation: When D=0: 0.5492424242424242. When D=1: 0.5125.\n"
          ]
        }
      ],
      "source": [
        "netflixclassifier = NaiveClassifer(\"/content/netflix-small-train.csv\", \"/content/netflix-test.csv\", ['3 Idiots', 'Bourne Identity', 'Bruce Almighty', 'Forest Gump',\n",
        "       'How to Lose a Guy in 10 Days', 'I Robot', 'Independence Day',\n",
        "       'La Vita E Bella', 'Lord of the Rings', 'Oceans 11', 'Patriot',\n",
        "       'Pearl Harbor', 'Pirates', 'Pulp Fiction', 'Rat Race', 'Shrek',\n",
        "       'Star Wars', 'What Women Want', 'When Harry Met Sally'])\n",
        "netflixpredict = netflixclassifier.predict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netflixclassifier.condprob0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-AxRzlprF1X",
        "outputId": "ca72dcdc-dd7c-4d44-9d68-a5646ea797da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'3 Idiots': [16, 0.6046511627906976],\n",
              " 'Bourne Identity': [10, 0.7441860465116279],\n",
              " 'Bruce Almighty': [18, 0.5581395348837209],\n",
              " 'Forest Gump': [16, 0.6046511627906976],\n",
              " 'How to Lose a Guy in 10 Days': [21, 0.4883720930232558],\n",
              " 'I Robot': [15, 0.627906976744186],\n",
              " 'Independence Day': [19, 0.5348837209302325],\n",
              " 'La Vita E Bella': [15, 0.627906976744186],\n",
              " 'Lord of the Rings': [6, 0.8372093023255814],\n",
              " 'Oceans 11': [9, 0.7674418604651163],\n",
              " 'Patriot': [13, 0.6744186046511628],\n",
              " 'Pearl Harbor': [16, 0.6046511627906976],\n",
              " 'Pirates': [7, 0.813953488372093],\n",
              " 'Pulp Fiction': [14, 0.6511627906976745],\n",
              " 'Rat Race': [23, 0.4418604651162791],\n",
              " 'Shrek': [20, 0.5116279069767442],\n",
              " 'Star Wars': [8, 0.7906976744186046],\n",
              " 'What Women Want': [23, 0.4418604651162791],\n",
              " 'When Harry Met Sally': [31, 0.2558139534883721]}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9UdbQgl6Skx"
      },
      "outputs": [],
      "source": [
        "def getpsetanswers(classifer):\n",
        "  train = pd.read_csv(classifer.traincsv)\n",
        "  print(f'The estimate for P(Y=1) is {classifer.yprobs[1]}.')\n",
        "\n",
        "  print(\"Values of P(X_i=1|Y=1) for all values of i:\")\n",
        "  for key in classifer.condprob1:\n",
        "    print(f'      {key}: {classifer.condprob1[key][1]}')\n",
        "\n",
        "  relprobs = {}\n",
        "  for key in classifer.condprob1:\n",
        "    relprobs[key] = (classifer.condprob1[key][1]*len(train[train[key] == 0]))/(classifer.condprob1[key][0]*len(train[train[key] == 1]))\n",
        "  \n",
        "  print(\"The top three indicative features were:\")\n",
        "  sortdict = sorted(relprobs, key=relprobs.get)\n",
        "  print(sortdict[-3:])\n",
        "  print(relprobs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ippm9XyV8EVd",
        "outputId": "9a6a659e-f0ad-481f-91f9-a44f49c84712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The estimate for P(Y=1) is 0.5.\n",
            "Values of P(X_i=1|Y=1) for all values of i:\n",
            "      A.1: 0.4523809523809524\n",
            "      A.2: 0.2619047619047619\n",
            "      A.3: 0.38095238095238093\n",
            "      A.4: 0.3333333333333333\n",
            "      B.1: 0.35714285714285715\n",
            "      B.2: 0.19047619047619047\n",
            "      B.3: 0.40476190476190477\n",
            "      B.4: 0.4523809523809524\n",
            "      B.5: 0.2857142857142857\n",
            "      C.1: 0.38095238095238093\n",
            "      C.2: 0.3333333333333333\n",
            "      C.3: 0.38095238095238093\n",
            "      C.4: 0.5714285714285714\n",
            "      C.5: 0.30952380952380953\n",
            "      D.1: 0.14285714285714285\n",
            "      D.2: 0.3333333333333333\n",
            "      D.3: 0.21428571428571427\n",
            "      D.4: 0.16666666666666666\n",
            "      E.1: 0.2619047619047619\n",
            "      E.2: 0.3333333333333333\n",
            "      E.3: 0.38095238095238093\n",
            "      E.4: 0.47619047619047616\n",
            "The top three indicative features were:\n",
            "['D.3', 'D.4', 'C.4']\n",
            "{'A.1': 1.4527736131934033, 'A.2': 1.8287841191066998, 'A.3': 1.7289377289377288, 'A.4': 1.8529411764705885, 'B.1': 1.296296296296296, 'B.2': 1.6470588235294117, 'B.3': 1.9104761904761904, 'B.4': 2.1778656126482208, 'B.5': 1.733333333333333, 'C.1': 1.5250836120401334, 'C.2': 2.0, 'C.3': 1.8461538461538458, 'C.4': 2.476190476190476, 'C.5': 1.793103448275862, 'D.1': 2.055555555555556, 'D.2': 2.3571428571428577, 'D.3': 2.454545454545454, 'D.4': 2.4666666666666663, 'E.1': 1.5376344086021505, 'E.2': 1.722222222222222, 'E.3': 1.9757085020242913, 'E.4': 1.8881118881118877}\n"
          ]
        }
      ],
      "source": [
        "getpsetanswers(heart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOeNtKJPQvTR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "class LogisticRegression:\n",
        "  parameters = []\n",
        "  traincsv = \"\"\n",
        "  testcsv = \"\"\n",
        "  cols = []\n",
        "  lr = 0\n",
        "  N = 1000\n",
        "  accuracy = 0\n",
        "\n",
        "  # initialize the parameters, and add the col of 1s\n",
        "  def __init__(self, train, test, col, lr, iters):\n",
        "    self.parameters = [0]*(len(col)+1)\n",
        "    self.traincsv = train\n",
        "    self.testcsv = test\n",
        "\n",
        "    self.cols = col\n",
        "    self.cols.append(\"theta_naught\")\n",
        "\n",
        "    self.accuracy = 0\n",
        "    self.lr = lr\n",
        "    self.N = iters\n",
        "    self.train()\n",
        "\n",
        "  #sigmoid function for later use\n",
        "  def sigmoid(self,x):\n",
        "    val = 1/(1+math.exp(-x))\n",
        "    return val\n",
        "\n",
        "  def train(self):\n",
        "    traindf = pd.read_csv(self.traincsv)\n",
        "\n",
        "    # add a column of 1s to make calculations for theta_0 more straightforward\n",
        "    traindf[\"theta_naught\"] = [1]*len(traindf)\n",
        "    traindfcut = traindf[self.cols] \n",
        "    \n",
        "    # set up iterations\n",
        "    for i in range(0,self.N):\n",
        "\n",
        "      #initialize gradient with all zeros at each iteration\n",
        "      gradient = [0]*len(self.parameters)\n",
        "      \n",
        "      # update the gradient for each row and for each feature in each row\n",
        "      for j in range(0,len(traindf)):\n",
        "        for k in range(0,len(self.cols)):\n",
        "          x  = np.dot(traindfcut.iloc[j], self.parameters)\n",
        "          gradient[k] += traindf.loc[j, self.cols[k]]*(traindf.loc[j,\"Label\"]-self.sigmoid(x))\n",
        "      \n",
        "      # update the parameters given the gradient calculated above\n",
        "      self.parameters = [x + self.lr*y for x, y in zip(self.parameters, gradient)]\n",
        "\n",
        "\n",
        "  def predict(self):\n",
        "      test = pd.read_csv(self.testcsv)\n",
        "      test[\"theta_naught\"] = [1]*len(test)\n",
        "      testcut = test[self.cols] \n",
        "\n",
        "      predictions = []\n",
        "\n",
        "      # using the previously calculated parameters, calcualte P(Y=y|x) using the sigmoid function\n",
        "      for i in range(0, len(test)):\n",
        "        \n",
        "        result = self.sigmoid(np.dot(testcut.iloc[i], self.parameters))\n",
        "\n",
        "        # if prob is less than 0.5, val is 0 and if greater than 0.5, val is 1\n",
        "        if result < 0.5 :\n",
        "          predictions.append(0)\n",
        "        else:\n",
        "          predictions.append(1)\n",
        "      \n",
        "      test[\"prediction\"] = predictions\n",
        "\n",
        "      #print confusion matrix and accuracy\n",
        "      cm = confusion_matrix(test[\"Label\"], predictions)\n",
        "\n",
        "      print(cm)\n",
        "      print(f'Accuracy is {sklearn.metrics.accuracy_score(test[\"Label\"], predictions)}')\n",
        "      self.accuracy = sklearn.metrics.accuracy_score(test[\"Label\"], predictions)\n",
        "      return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfcysBu1ytI6"
      },
      "outputs": [],
      "source": [
        "simpleclassifier = LogisticRegression(\"/content/simple-train.csv\", \"/content/simple-test.csv\", ['x1', 'x2'], 0.0001, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMh4AiHc2myT",
        "outputId": "4d617b0a-edf0-42bb-a99a-94283b0d6c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2 0]\n",
            " [0 2]]\n",
            "Accuracy is 1.0\n"
          ]
        }
      ],
      "source": [
        "result = simpleclassifier.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMpPuzV4D4nF",
        "outputId": "6108a96a-c438-421d-a74d-002b33d372f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.09759347169504434, -0.0011685590390117962, -0.002356868930802323]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simpleclassifier.parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9PgTewB7kzS"
      },
      "outputs": [],
      "source": [
        "netflixclassifier = LogisticRegression(\"/content/netflix-small-train.csv\", \"/content/netflix-test.csv\", ['3 Idiots', 'Bourne Identity', 'Bruce Almighty', 'Forest Gump',\n",
        "       'How to Lose a Guy in 10 Days', 'I Robot', 'Independence Day',\n",
        "       'La Vita E Bella', 'Lord of the Rings', 'Oceans 11', 'Patriot',\n",
        "       'Pearl Harbor', 'Pirates', 'Pulp Fiction', 'Rat Race', 'Shrek',\n",
        "       'Star Wars', 'What Women Want', 'When Harry Met Sally'], 0.00625, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxlObTN67t7m",
        "outputId": "8b6dc3b6-7b84-4108-be53-35e68c80801e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[139 126]\n",
            " [ 43 192]]\n",
            "Accuracy is 0.662\n"
          ]
        }
      ],
      "source": [
        "netflixresults = netflixclassifier.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH2kiB-a9e-B"
      },
      "outputs": [],
      "source": [
        "def newpsetans(logistic):\n",
        "  print(\"The three most indicative features were:\")\n",
        "\n",
        "  newlist = zip(logistic.cols[:-1],logistic.parameters[:-1])\n",
        "\n",
        "  dictt = {}\n",
        "  for key, value in newlist:\n",
        "    dictt[key] = value\n",
        "    \n",
        "  sortdict = sorted(dictt, key=dictt.get)\n",
        "  print(sortdict[-3:])\n",
        "\n",
        "  train = pd.read_csv(logistic.traincsv)\n",
        "  train[\"theta_naught\"] = [1]*len(train)\n",
        "  traincut = train[logistic.cols] \n",
        "  empty = [0]* len(logistic.cols)\n",
        "\n",
        "\n",
        "  ll0 = 0\n",
        "\n",
        "  for i in range(len(traincut)):\n",
        "    ll0 += train.loc[i,\"Label\"] * np.log((logistic.sigmoid(np.dot(empty,traincut.iloc[i])))) + (1-train.loc[i,\"Label\"])*np.log(1-(logistic.sigmoid(np.dot(empty,traincut.iloc[i]))))\n",
        "\n",
        "  lla = 0\n",
        "\n",
        "  for i in range(len(traincut)):\n",
        "    lla += train.loc[i,\"Label\"] * np.log((logistic.sigmoid(np.dot(logistic.parameters,traincut.iloc[i])))) + (1-train.loc[i,\"Label\"])*np.log(1-(logistic.sigmoid(np.dot(logistic.parameters,traincut.iloc[i]))))\n",
        "\n",
        "  print(f'Log Liklihood with parameters of 0: {ll0}.')\n",
        "  print(f'Log Liklihood with trained parameters: {lla}.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCgJl-31B3a6",
        "outputId": "8ae382fe-a7b2-4a6e-db9b-8887a52daf06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The three most indicative features were:\n",
            "['Rat Race', 'How to Lose a Guy in 10 Days', 'When Harry Met Sally']\n",
            "Log Liklihood with parameters of 0: -69.31471805599459.\n",
            "Log Liklihood with trained parameters: -44.53409947084053.\n"
          ]
        }
      ],
      "source": [
        "newpsetans(netflixclassifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3097D4dAtiV",
        "outputId": "88c5e720-76d1-4a6b-d009-10d567d112f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 13   2]\n",
            " [ 57 115]]\n",
            "Accuracy is 0.6844919786096256\n",
            "Learning Rate is 0.1, accuracy is 0.6844919786096256.\n",
            "[[ 13   2]\n",
            " [ 52 120]]\n",
            "Accuracy is 0.7112299465240641\n",
            "Learning Rate is 0.01, accuracy is 0.7112299465240641.\n",
            "[[ 12   3]\n",
            " [ 39 133]]\n",
            "Accuracy is 0.7754010695187166\n",
            "Learning Rate is 0.001, accuracy is 0.7754010695187166.\n",
            "[[  9   6]\n",
            " [ 29 143]]\n",
            "Accuracy is 0.8128342245989305\n",
            "Learning Rate is 0.0001, accuracy is 0.8128342245989305.\n",
            "[[  7   8]\n",
            " [  4 168]]\n",
            "Accuracy is 0.9358288770053476\n",
            "Learning Rate is 1e-05, accuracy is 0.9358288770053476.\n"
          ]
        }
      ],
      "source": [
        "# testing out different learning rates and how they fare\n",
        "for i in range(0,5):\n",
        "  lr  = 0.1 / 10**i\n",
        "  heart = LogisticRegression(\"/content/heart-train.csv\", \"/content/heart-test.csv\",['A.1', 'A.2', 'A.3', 'A.4', 'B.1', 'B.2', 'B.3', 'B.4', 'B.5', 'C.1',\n",
        "       'C.2', 'C.3', 'C.4', 'C.5', 'D.1', 'D.2', 'D.3', 'D.4', 'E.1', 'E.2',\n",
        "       'E.3', 'E.4'],lr,1000)\n",
        "  heartpredict = heart.predict()\n",
        "  print(f'Learning Rate is {lr}, accuracy is {heart.accuracy}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC0Ah74_A8rX",
        "outputId": "5e9d4339-c1bc-4edc-c18e-0dfc780be2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[98 11]\n",
            " [31 44]]\n",
            "Accuracy is 0.7717391304347826\n"
          ]
        }
      ],
      "source": [
        "ancestry = LogisticRegression(\"/content/ancestry-train.csv\", \"/content/newancestry.csv\", ['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8',\n",
        "       'col9', 'col10', 'col11', 'col12', 'col13', 'col14', 'col15', 'col16',\n",
        "       'col17', 'col18', 'col19'],0.0001,1000)\n",
        "results = ancestry.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUpQL8UUBNWw",
        "outputId": "5b7f88a4-33e5-4cdd-f52e-60749c70f7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three most indicative features were:\n",
            "['col12', 'col6', 'col10']\n",
            "Log Liklihood with parameters of 0: -196.1606520984649.\n",
            "Log Liklihood with trained parameters: -105.42099699171574.\n"
          ]
        }
      ],
      "source": [
        "newpsetans(ancestry)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cs109-pset6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}